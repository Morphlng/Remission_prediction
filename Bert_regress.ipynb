{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use only one GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pre-trained bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_NAME = \"hfl/chinese-roberta-wwm-ext-large\" # \"hfl/chinese-roberta-wwm-ext-large\" requires at least 16GB \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# By setting num_labels to 1 will automatically enable regression mode\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/train.csv\"\n",
    "data = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_truncate(text):\n",
    "    if len(text) <= 510:\n",
    "        return text\n",
    "    \n",
    "    return text[-511:-1]\n",
    "\n",
    "data['fact'] = [tail_truncate(t) for t in data.fact.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_30893</th>\n",
       "      <td>罪犯张都树，男，1994年xx月xx日出生，汉族，山西省浑源人，现在山西省太原第三监狱服刑，...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_45279</th>\n",
       "      <td>告人池杰犯运输毒品罪，判处有期徒刑十一年，并处罚金人民币一万元（已缴纳）。刑期自2017年x...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_16398</th>\n",
       "      <td>教所服刑，以被告人林凯利犯盗窃罪，判处有期徒刑八年，并处罚金人民币二万。服刑期间，发现漏罪。...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_13653</th>\n",
       "      <td>二年和并处罚金人民币五万元不变；于2019年xx月xx日作出（2019）桂02刑更845号刑...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_13748</th>\n",
       "      <td>x日投送扎兰屯监狱服刑。执行过程中，刑期无变动。截止2018年xx月xx日，剩余刑期十个月十...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_11284</th>\n",
       "      <td>伤害罪，判处有期徒刑三年六个月。该判决已发生法律效力，罪犯李伟于2017年xx月xx日入监服...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_44732</th>\n",
       "      <td>建议书报送本院审理。本院依法组成合议庭进行了审理，现已审理终结。 执行机关提出，罪犯欧九固在...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_38158</th>\n",
       "      <td>罪犯姜群，男，1971年xx月xx日出生，汉族，出生地辽宁省沈阳市，初中文化，现在辽宁省沈阳...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_860</th>\n",
       "      <td>1年xx月xx日起至2021年xx月xx日止。判决发生法律效力后于2012年xx月xx日交付...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_15795</th>\n",
       "      <td>月。2014年xx月xx日本院裁定减去有期徒刑一年一个月，剥夺政治权利八年不变。现执行机关黑...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       fact  label\n",
       "id                                                                \n",
       "id_30893  罪犯张都树，男，1994年xx月xx日出生，汉族，山西省浑源人，现在山西省太原第三监狱服刑，...      8\n",
       "id_45279  告人池杰犯运输毒品罪，判处有期徒刑十一年，并处罚金人民币一万元（已缴纳）。刑期自2017年x...      7\n",
       "id_16398  教所服刑，以被告人林凯利犯盗窃罪，判处有期徒刑八年，并处罚金人民币二万。服刑期间，发现漏罪。...     12\n",
       "id_13653  二年和并处罚金人民币五万元不变；于2019年xx月xx日作出（2019）桂02刑更845号刑...      6\n",
       "id_13748  x日投送扎兰屯监狱服刑。执行过程中，刑期无变动。截止2018年xx月xx日，剩余刑期十个月十...      7\n",
       "...                                                     ...    ...\n",
       "id_11284  伤害罪，判处有期徒刑三年六个月。该判决已发生法律效力，罪犯李伟于2017年xx月xx日入监服...      0\n",
       "id_44732  建议书报送本院审理。本院依法组成合议庭进行了审理，现已审理终结。 执行机关提出，罪犯欧九固在...      9\n",
       "id_38158  罪犯姜群，男，1971年xx月xx日出生，汉族，出生地辽宁省沈阳市，初中文化，现在辽宁省沈阳...      9\n",
       "id_860    1年xx月xx日起至2021年xx月xx日止。判决发生法律效力后于2012年xx月xx日交付...     12\n",
       "id_15795  月。2014年xx月xx日本院裁定减去有期徒刑一年一个月，剥夺政治权利八年不变。现执行机关黑...      5\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(data, train_size=0.8, random_state=42)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text\n",
    "train_encodings = tokenizer(train_data.fact.tolist(), truncation=True, padding=True, max_length=512)\n",
    "valid_encodings = tokenizer(val_data.fact.tolist(), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it torch friendly\n",
    "class TorchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels = None, length = None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        # If predicting then no label given\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "            item[\"labels\"] = float(item[\"labels\"])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        \n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = TorchData(train_encodings, train_data.label)\n",
    "valid_dataset = TorchData(valid_encodings, val_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_acc(labels, logits):\n",
    "    logits = np.round(logits)\n",
    "    cnt = 0\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == logits[idx]:\n",
    "            cnt += 1\n",
    "    return cnt / len(labels)\n",
    "\n",
    "def v_standard(labels, logits):\n",
    "    logits = np.round(logits)\n",
    "    v_vec = np.abs(np.log(logits + 1) - np.log(labels + 1))\n",
    "    \n",
    "    res = []\n",
    "    for v in v_vec:\n",
    "        if v <= 0.2:\n",
    "            res.append(1)\n",
    "        elif v <= 0.4:\n",
    "            res.append(0.8)\n",
    "        elif v <= 0.6:\n",
    "            res.append(0.6)\n",
    "        elif v <=0.8:\n",
    "            res.append(0.4)\n",
    "        elif v <= 1.0:\n",
    "            res.append(0.2)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def final_score(labels, logits):\n",
    "    ext_acc = exact_acc(labels, logits)\n",
    "    v = v_standard(labels, logits)\n",
    "    return np.sum(v) * 0.7 + ext_acc * 0.3\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    v = v_standard(labels, logits)\n",
    "    ext_acc = exact_acc(labels, logits)\n",
    "    score = np.sum(v) * 0.7 + ext_acc * 0.3 # final_score(labels, logits)\n",
    "\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"v\": v_standard, \"ext_acc\": ext_acc, \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',          \n",
    "    num_train_epochs = num_epochs,     \n",
    "    per_device_train_batch_size = 8,  # Can't be too big \n",
    "    per_device_eval_batch_size = 8,   \n",
    "    weight_decay = 0.01,               \n",
    "    learning_rate = 2e-5,\n",
    "    logging_dir = './logs',            \n",
    "    save_total_limit = 2,   # By setting this, we only save best and last model\n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'score',    \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\"\n",
    ")\n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = valid_dataset,          \n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/best\n",
      "Configuration saved in results/best/config.json\n",
      "Model weights saved in results/best/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"results/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10001\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7279f8ed987e4570b1e44b8f4f4780f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.4907639026641846,\n",
       " 'eval_mse': 3.4907641410827637,\n",
       " 'eval_rmse': 1.8683587312698364,\n",
       " 'eval_mae': 1.0228413343429565,\n",
       " 'eval_r2': 0.8383862463467547,\n",
       " 'eval_score': 6551.873554644536,\n",
       " 'eval_runtime': 245.2143,\n",
       " 'eval_samples_per_second': 40.785,\n",
       " 'eval_steps_per_second': 5.102,\n",
       " 'epoch': 52.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Result for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = 'results/best/pytorch_model.bin'\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('data/testA.csv')\n",
    "\n",
    "test_set['fact'] = [tail_truncate(t) for t in test_set.fact.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(test_set.fact.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "test_dataset = TorchData(encodings, length=25001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 25001\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c56bbf45b44095adb5a146f3e21a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对测试数据预测，结果为浮点数\n",
    "pred_labels = trainer.predict(test_dataset)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('data/submission.csv')\n",
    "id_label_list = []\n",
    "\n",
    "for index,row in submit.iterrows():\n",
    "    idx = int(row['id'].replace(\"id_\", '')) - 50000\n",
    "    label = int(np.round(pred_labels[idx][0]))\n",
    "    id_label_list.append([row['id'], label])\n",
    "\n",
    "df = pd.DataFrame(data=id_label_list, columns=['id','label'])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff05c44de0e0b0818a1c7c068b364f8d3f785848e0c17900ee478442c38b7cd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
